{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be92d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda21b94",
   "metadata": {},
   "source": [
    "### Google Gemini Model Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea06dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"google_genai:gemini-2.5-flash\")\n",
    "response = model.invoke(\"Why do parrots talk?\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f810d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n",
    "response = model.invoke(\"Hello, how are you?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b51c4f",
   "metadata": {},
   "source": [
    "### GROQ MODEL Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82808132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"groq:qwen/qwen3-32b\")\n",
    "response = model.invoke(\"Hello, how are you?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "response = model.invoke(\"Hello, how are you?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe0d04b",
   "metadata": {},
   "source": [
    "## Streaming And Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c564056",
   "metadata": {},
   "source": [
    "### Streaming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3564663d",
   "metadata": {},
   "source": [
    "Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses. Calling stream() returns an iterator that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51660890",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\"Write me a 200 words paragraph on Artificial Intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b546df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in model.stream(\"Write me a 200 words paragraph on Artificial Intelligence\"):\n",
    "    print(chunk.text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486eb351",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\"Why do parrots have colorful feathers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n",
    "    print(chunk.text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508285fc",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4a2cf",
   "metadata": {},
   "source": [
    "Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a20c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = model.batch([\n",
    "    \"Why do parrots have colorful feathers?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"What is quantum computing?\"\n",
    "])\n",
    "for response in responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.batch(\n",
    "    [\"Why do parrots have colorful feathers?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"What is quantum computing?\"],\n",
    "    config={\n",
    "        'max_concurrency': 5,  # Limit to 5 parallel calls\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain_genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
